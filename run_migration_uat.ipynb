{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd2a8e8",
   "metadata": {},
   "source": [
    "# Data Migration: SQL to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a3c1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e52d2e",
   "metadata": {},
   "source": [
    "## 1. Load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "290222ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc85a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_host = os.getenv(\"SQL_SERVER_HOST\")\n",
    "sql_db = os.getenv(\"SQL_SERVER_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3086af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_host = os.getenv(\"POSTGRES_HOST\") \n",
    "pg_port = os.getenv(\"POSTGRES_PORT\")\n",
    "pg_db = os.getenv(\"POSTGRES_DB\")\n",
    "pg_user = os.getenv(\"POSTGRES_USER\")\n",
    "pg_password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bdac07a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSTGRES_HOST: localhost\n",
      "POSTGRES_PORT: 5432\n",
      "POSTGRES_DB: transaction_uat\n",
      "POSTGRES_USER: postgres\n",
      "POSTGRES_PASSWORD: postgres\n"
     ]
    }
   ],
   "source": [
    "print(f\"POSTGRES_HOST: {pg_host}\")\n",
    "print(f\"POSTGRES_PORT: {pg_port}\")\n",
    "print(f\"POSTGRES_DB: {pg_db}\")\n",
    "print(f\"POSTGRES_USER: {pg_user}\")\n",
    "print(f\"POSTGRES_PASSWORD: {pg_password}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7e269",
   "metadata": {},
   "source": [
    "## 2. Connect to SQL Server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cb7a3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to SQL Server...\n",
      "   Server: INTELI5SSD-LAPT\\SQLEXPRESS\n",
      "   Database: TransactionDB_UAT\n"
     ]
    }
   ],
   "source": [
    "print (\"Connecting to SQL Server...\")\n",
    "print (f\"   Server: {sql_host}\")\n",
    "print (f\"   Database: {sql_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3dab0f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] -> Connection to SQL Server completed \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sql_conn_string = (\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={sql_host};\"\n",
    "    f\"DATABASE={sql_db};\"\n",
    "    f\"Trusted_Connection=yes;\"\n",
    "    )\n",
    "\n",
    "    sql_conn = pyodbc.connect(sql_conn_string)\n",
    "    sql_cursor = sql_conn.cursor()\n",
    "    print (\"[SUCCESS] -> Connection to SQL Server completed \")\n",
    "except Exception as e:\n",
    "    print(f\"SQl Server connection failed {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb14137",
   "metadata": {},
   "source": [
    "## 3. Coonect to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8e27c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Postgres...\n",
      "  Server: localhost\n",
      "  Database: transaction_uat\n"
     ]
    }
   ],
   "source": [
    "print (\"Connecting to Postgres...\")\n",
    "print (f\"  Server: {pg_host}\")\n",
    "print (f\"  Database: {pg_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9f4ab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to postgres\n",
      "   Version: PostgreSQL 18.1 on x86_64-windows, compiled by msv...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pg_conn = psycopg2.connect(\n",
    "        host=pg_host,\n",
    "        port=pg_port,\n",
    "        database=pg_db,\n",
    "        user=pg_user,\n",
    "        password=pg_password\n",
    "    )\n",
    "\n",
    "    pg_cursor=pg_conn.cursor()\n",
    "    pg_cursor.execute(\"SELECT version();\")\n",
    "    pg_version = pg_cursor.fetchone()[0]\n",
    "\n",
    "    print (\"Connected to postgres\")\n",
    "    print (f\"   Version: {pg_version[:50]}...\\n\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    print(f\"  Postgres connection failed: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print (f\"unexpected error {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6093ccd",
   "metadata": {},
   "source": [
    "## Define the tables to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ce83ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  these are the tables: ['Categories', 'Suppliers', 'Customers', 'Products']\n"
     ]
    }
   ],
   "source": [
    "tables_to_migrate = ['Categories', 'Suppliers', 'Customers', 'Products']\n",
    "print(f\"  these are the tables: {tables_to_migrate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce2b7e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.Categories\n",
      "  2.Suppliers\n",
      "  3.Customers\n",
      "  4.Products\n",
      "\n",
      " The total number of tables to migrate :4\n"
     ]
    }
   ],
   "source": [
    "for i, table in enumerate(tables_to_migrate, 1):\n",
    "    print(f\"  {i}.{table}\")\n",
    "Total_tables_to_migrate = len(tables_to_migrate)    \n",
    "print (f\"\\n The total number of tables to migrate :{Total_tables_to_migrate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37237a2",
   "metadata": {},
   "source": [
    "## Run pre-migration checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c3000b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> ROW COUNTS\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> ROW COUNTS\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0e2c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total customers is: 900000\n"
     ]
    }
   ],
   "source": [
    "customers = \"SELECT COUNT(*) AS TotalCustomer FROM Customers;\"\n",
    "sql_cursor.execute(customers)\n",
    "customers_count = sql_cursor.fetchone()[0]\n",
    "print(f\" Total customers is: {customers_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29fe88b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories                     8 rows\n",
      "Suppliers                      5000 rows\n",
      "Customers                      900000 rows\n",
      "Products                       150000 rows\n",
      "---------------------------------------------\n",
      "Total                         1,055,008 rows\n"
     ]
    }
   ],
   "source": [
    "baseline_counts = {}\n",
    "\n",
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "        row_count_query = f\"SELECT COUNT(*) FROM {table}\"\n",
    "        sql_cursor.execute(row_count_query)\n",
    "        count = sql_cursor.fetchone()[0]\n",
    "\n",
    "        baseline_counts[table] = count\n",
    "        print(f\"{table:30} {count} rows\")\n",
    "    print(\"-\" * 45)    \n",
    "    total_count = sum(baseline_counts.values())\n",
    "    print(f\"{'Total':28} {total_count:>10,} rows\")\n",
    "except Exception as e:\n",
    "    print (f\"Failed to get baseline counts: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7a542f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHECK 2: NULL CHECKS (CustomerName)\n",
      "\n",
      "CHECK 3: INVALID EMAIL FORMATS\n",
      "\n",
      "CHECK 4: NEGATIVE PRODUCT PRICES\n",
      "\n",
      "CHECK 5: NEGATIVE STOCK QUANTITY\n",
      "\n",
      "CHECK 6: IDENTIFYING ORPHAN RECORDS\n",
      "\n",
      "CHECK 7: FUTURE DATE CHECK\n",
      "\n",
      "DATA QUALITY ISSUES FOUND:\n",
      " 4,514 Customers with null names...\n",
      " 8844 invalid email formats...\n",
      " 775 Negative prices...\n",
      " 1,467 Negative stocks...\n",
      " 24,700 Orphan records...\n",
      " 5,184 future date records...\n"
     ]
    }
   ],
   "source": [
    "data_quality_issues = []\n",
    "print(\"\\nCHECK 2: NULL CHECKS (CustomerName)\")\n",
    "try:\n",
    "    customer_query = \"\"\"SELECT COUNT(*) AS null_count\n",
    "                        FROM Customers \n",
    "                        WHERE CustomerName is NULL\"\"\"\n",
    "    sql_cursor.execute(customer_query)\n",
    "    null_customer_count= sql_cursor.fetchone()[0]\n",
    "    if null_customer_count > 0:\n",
    "        data_quality_issues.append(f\" {null_customer_count:,} Customers with null names...\")\n",
    "    #print (f\"Total null customer is:{data_quality_issues[0]:,}...\")\n",
    "\n",
    "    print(\"\\nCHECK 3: INVALID EMAIL FORMATS\")\n",
    "    sql_cursor.execute(\"\"\"SELECT COUNT(*) AS invalid_email_count\n",
    "                            FROM Customers\n",
    "                            WHERE Email LIKE '%@invalid'  \"\"\")\n",
    "    invalid_emails = sql_cursor.fetchone()[0]\n",
    "    if invalid_emails > 0:\n",
    "        data_quality_issues.append(f\" {invalid_emails} invalid email formats...\")\n",
    "        #print (data_quality_issues)\n",
    "\n",
    "    print(\"\\nCHECK 4: NEGATIVE PRODUCT PRICES\")\n",
    "    sql_cursor.execute(\"\"\"SELECT COUNT(*) AS negative_price_count\n",
    "                            FROM Products\n",
    "                            WHERE UnitPrice < 0; \"\"\")\n",
    "    negative_prices = sql_cursor.fetchone()[0]\n",
    "    if negative_prices > 0:\n",
    "        data_quality_issues.append(f\" {negative_prices:,} Negative prices...\")\n",
    "        #print (data_quality_issues)\n",
    "\n",
    "    print(\"\\nCHECK 5: NEGATIVE STOCK QUANTITY\")\n",
    "    sql_cursor.execute(\"\"\"SELECT COUNT(*) AS negative_stock_count\n",
    "                            FROM Products\n",
    "                            WHERE StockQuantity < 0; \"\"\")\n",
    "    negative_stock = sql_cursor.fetchone()[0]\n",
    "    if negative_stock > 0:\n",
    "        data_quality_issues.append(f\" {negative_stock:,} Negative stocks...\")\n",
    "        #print (data_quality_issues)\n",
    "\n",
    "    print(\"\\nCHECK 6: IDENTIFYING ORPHAN RECORDS\")    \n",
    "    sql_cursor.execute(\"\"\"SELECT COUNT(*)\n",
    "                       FROM Products prod\n",
    "                       WHERE NOT EXISTS (SELECT 1\n",
    "                       FROM Suppliers sup\n",
    "                       WHERE sup.SupplierID=prod.SupplierID) \"\"\")\n",
    "    orpthan_data = sql_cursor.fetchone()[0]\n",
    "    if orpthan_data > 0:\n",
    "        data_quality_issues.append(f\" {orpthan_data:,} Orphan records...\")\n",
    "        #print (data_quality_issues)\n",
    "\n",
    "    print(\"\\nCHECK 7: FUTURE DATE CHECK\")    \n",
    "    sql_cursor.execute(\"\"\"SELECT COUNT(*)AS future_date_count\n",
    "                       FROM Customers\n",
    "                       WHERE CreatedDate > GETDATE() \"\"\")\n",
    "    future_dates = sql_cursor.fetchone()[0]\n",
    "    if future_dates > 0:\n",
    "        data_quality_issues.append(f\" {future_dates:,} future date records...\")\n",
    "        #print (data_quality_issues)    \n",
    "\n",
    "    if data_quality_issues:\n",
    "        print (\"\\nDATA QUALITY ISSUES FOUND:\")\n",
    "        for quality_issues in data_quality_issues:\n",
    "            print(quality_issues)\n",
    "    else:\n",
    "        print(\"No data quality issues identified\")\n",
    "\n",
    "except Exception (e):\n",
    "    print(f\"[ERROR]====> {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5d530fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_29484\\3533684230.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_29484\\3533684230.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_29484\\3533684230.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_29484\\3533684230.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0    CategoryID       int                       NaN          NO\n",
      "1  CategoryName  nvarchar                      50.0         YES\n",
      "2   Description  nvarchar                      -1.0         YES\n",
      "\n",
      "Categories\n",
      "    COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0    SupplierID       int                       NaN          NO\n",
      "1  SupplierName  nvarchar                     150.0         YES\n",
      "2   ContactName  nvarchar                     100.0         YES\n",
      "3       Country  nvarchar                     100.0         YES\n",
      "4         Phone  nvarchar                      20.0         YES\n",
      "\n",
      "Suppliers\n",
      "    COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0    CustomerID       int                       NaN          NO\n",
      "1  CustomerName  nvarchar                     100.0         YES\n",
      "2         Email  nvarchar                     100.0         YES\n",
      "3         Phone  nvarchar                      20.0         YES\n",
      "4       Country  nvarchar                     100.0         YES\n",
      "5   CreatedDate  datetime                       NaN         YES\n",
      "6      IsActive       bit                       NaN         YES\n",
      "\n",
      "Customers\n",
      "     COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0      ProductID       int                       NaN          NO\n",
      "1    ProductName  nvarchar                     200.0         YES\n",
      "2     CategoryID       int                       NaN         YES\n",
      "3     SupplierID       int                       NaN         YES\n",
      "4      UnitPrice     money                       NaN         YES\n",
      "5  StockQuantity       int                       NaN         YES\n",
      "6    CreatedDate  datetime                       NaN         YES\n",
      "\n",
      "Products\n"
     ]
    }
   ],
   "source": [
    "table_schema = {}\n",
    "\n",
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "        schema_query = f\"\"\"\n",
    "            SELECT\n",
    "                COLUMN_NAME,\n",
    "                DATA_TYPE,\n",
    "                CHARACTER_MAXIMUM_LENGTH,\n",
    "                IS_NULLABLE\n",
    "            FROM\n",
    "                INFORMATION_SCHEMA.COLUMNS\n",
    "            WHERE\n",
    "                table_name = '{table}'\n",
    "            ORDER BY\n",
    "                ORDINAL_POSITION\n",
    "            \"\"\"\n",
    "        schema_df = pd.read_sql(schema_query, sql_conn)\n",
    "        print(schema_df)\n",
    "        table_schema[table] = schema_df\n",
    "        #print(schema_df)\n",
    "        print(f\"\\n{table}\")\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540976b",
   "metadata": {},
   "source": [
    "## 7. Define data type mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d3eb33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlserver_to_postgres_types_mapping = {\n",
    "    'int': 'INTEGER',\n",
    "    'bigint': 'BIGINT',\n",
    "    'smallint': 'SMALLINT',\n",
    "    'tinyint': 'SMALLINT',\n",
    "    'bit': 'BOOLEAN',\n",
    "    'decimal': 'NUMERIC',\n",
    "    'numeric': 'NUMERIC',\n",
    "    'money': 'NUMERIC(19,4)',\n",
    "    'smallmoney': 'NUMERIC(10,4)',\n",
    "    'float': 'DOUBLE PRECISION',\n",
    "    'real': 'REAL',\n",
    "    'datetime': 'TIMESTAMP',\n",
    "    'datetime2': 'TIMESTAMP',\n",
    "    'smalldatetime': 'TIMESTAMP',\n",
    "    'date': 'DATE',\n",
    "    'time': 'TIME',\n",
    "    'char': 'CHAR',\n",
    "    'varchar': 'VARCHAR',\n",
    "    'nchar': 'CHAR',\n",
    "    'nvarchar': 'VARCHAR',\n",
    "    'text': 'TEXT',\n",
    "    'ntext': 'TEXT'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48f8cc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Server to PostgreSQL type mapping \n",
      "  int             --->      INTEGER\n",
      "  bigint          --->      BIGINT\n",
      "  smallint        --->      SMALLINT\n",
      "  tinyint         --->      SMALLINT\n",
      "  bit             --->      BOOLEAN\n",
      "  decimal         --->      NUMERIC\n",
      "  numeric         --->      NUMERIC\n",
      "  money           --->      NUMERIC(19,4)\n",
      "  smallmoney      --->      NUMERIC(10,4)\n",
      "  float           --->      DOUBLE PRECISION\n",
      "  real            --->      REAL\n",
      "  datetime        --->      TIMESTAMP\n",
      "  datetime2       --->      TIMESTAMP\n",
      "  smalldatetime   --->      TIMESTAMP\n",
      "  date            --->      DATE\n",
      "  time            --->      TIME\n",
      "  char            --->      CHAR\n",
      "  varchar         --->      VARCHAR\n",
      "  nchar           --->      CHAR\n",
      "  nvarchar        --->      VARCHAR\n",
      "  text            --->      TEXT\n",
      "  ntext           --->      TEXT\n"
     ]
    }
   ],
   "source": [
    "print(\"SQL Server to PostgreSQL type mapping \")\n",
    "for sql_type, pg_type in sqlserver_to_postgres_types_mapping.items():\n",
    "    print(f\"  {sql_type:15} --->      {pg_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd0662",
   "metadata": {},
   "source": [
    "## 8. Create tables in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9ea1176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " + =======================================================\n",
      "[SUCCESS]---> All tables created successfully\n",
      "['categoryid SERIAL PRIMARY KEY', 'categoryname VARCHAR', 'description VARCHAR']\n",
      "\n",
      " + =======================================================\n",
      "[SUCCESS]---> All tables created successfully\n",
      "['supplierid SERIAL PRIMARY KEY', 'suppliername VARCHAR', 'contactname VARCHAR', 'country VARCHAR', 'phone VARCHAR']\n",
      "\n",
      " + =======================================================\n",
      "[SUCCESS]---> All tables created successfully\n",
      "['customerid SERIAL PRIMARY KEY', 'customername VARCHAR', 'email VARCHAR', 'phone VARCHAR', 'country VARCHAR', 'createddate TIMESTAMP', 'isactive BOOLEAN']\n",
      "\n",
      " + =======================================================\n",
      "[SUCCESS]---> All tables created successfully\n",
      "['productid SERIAL PRIMARY KEY', 'productname VARCHAR', 'categoryid INTEGER', 'supplierid INTEGER', 'unitprice NUMERIC(19,4)', 'stockquantity INTEGER', 'createddate TIMESTAMP']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "        schema = table_schema[table]\n",
    "        pg_table = table.lower()\n",
    "        #print(pg_table)\n",
    "        pg_cursor.execute(f\"DROP TABLE IF EXISTS {pg_table} CASCADE\")\n",
    "\n",
    "        column_definitions = []\n",
    "\n",
    "        for idx, row in schema.iterrows():\n",
    "            col_name = row['COLUMN_NAME'].lower()\n",
    "            sql_type = row['DATA_TYPE']\n",
    "\n",
    "            base_type = sql_type.lower()\n",
    "            pg_type = sqlserver_to_postgres_types_mapping.get(base_type, 'TEXT')\n",
    "\n",
    "            condition_1 = idx == 0\n",
    "            condition_2 = col_name.endswith('id')\n",
    "            condition_3 = 'int' in sql_type.lower()\n",
    "\n",
    "            if condition_1 and condition_2 and condition_3:\n",
    "                column_definitions.append(f\"{col_name} SERIAL PRIMARY KEY\")\n",
    "            else:\n",
    "                column_definitions.append(f\"{col_name} {pg_type}\")\n",
    "        \n",
    "        column_string = \",\\n    \".join(column_definitions)\n",
    "        create_query = f\"\"\"\n",
    "        CREATE TABLE {pg_table} (\n",
    "            {column_string}\n",
    "        )\n",
    "        \"\"\"\n",
    "        pg_cursor.execute(create_query)\n",
    "        pg_conn.commit()\n",
    "\n",
    "        print(\"\\n + \"  + \"=\" * 55)\n",
    "        print (\"[SUCCESS]---> All tables created successfully\")\n",
    "        print(column_definitions)\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Postgres experience an error while creating a table: {e}\")\n",
    "    pg_conn.rollback()\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected issue: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fad868f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table = 'Customers'\n",
    "pg_table = test_table.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82925c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Read from SQL Server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_29484\\3998754814.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  test_df = pd.read_sql(extract_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 900,000 rows\n",
      "2. Transforming data types...\n",
      "[SUCCESS] ---> Converted IsActive: BIT ---> BOOLEAN\n",
      "3. Prepare the data for loading...\n",
      "Prepared 900,000 rows\n",
      "4. Insert data into PostgreSQL...\n",
      "Loaded 900,000 rows\n",
      "5. Verifying...\n",
      "[SUCCESS] ---> Verification passed: 900,000 == 900,000\n",
      "\n",
      "Customers migration test successfully completed!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"1. Read from SQL Server...\")\n",
    "    extract_query = f\"SELECT * FROM {pg_table}\"\n",
    "    test_df = pd.read_sql(extract_query, sql_conn)\n",
    "    print(f\"Read {len(test_df):,} rows\")\n",
    "\n",
    "    print(\"2. Transforming data types...\")\n",
    "    if \"IsActive\" in test_df.columns:\n",
    "        test_df[\"IsActive\"] = test_df[\"IsActive\"].astype(bool)\n",
    "        print(\"[SUCCESS] ---> Converted IsActive: BIT ---> BOOLEAN\")\n",
    "\n",
    "    print(\"3. Prepare the data for loading...\")\n",
    "    data_tuples = [tuple(row) for row in test_df.to_numpy()]\n",
    "    columns = [col.lower() for col in test_df.columns]\n",
    "    columns_string = \", \".join(columns)\n",
    "    placeholders = \", \".join([\"%s\"] * len(columns))\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "        INSERT INTO {pg_table} ({columns_string})\n",
    "        VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Prepared {len(data_tuples):,} rows\")\n",
    "\n",
    "    print(\"4. Insert data into PostgreSQL...\")\n",
    "    execute_values(pg_cursor, insert_query, data_tuples, page_size=1000)\n",
    "    pg_conn.commit()\n",
    "    print(f\"Loaded {len(data_tuples):,} rows\")\n",
    "\n",
    "    print(\"5. Verifying...\")\n",
    "    pg_cursor.execute(f\"SELECT COUNT(*) FROM {pg_table}\")\n",
    "    pg_count = pg_cursor.fetchone()[0]\n",
    "\n",
    "    sql_count = baseline_counts[test_table]\n",
    "\n",
    "    if pg_count == sql_count:\n",
    "        print(f\"[SUCCESS] ---> Verification passed: {pg_count:,} == {sql_count:,}\")\n",
    "    else:\n",
    "        print(f\"[FAILED] ---> Count mismatch: {pg_count:,} != {sql_count:,}\")\n",
    "\n",
    "    print(f\"\\n{test_table} migration test successfully completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    pg_conn.rollback()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616400fd",
   "metadata": {},
   "source": [
    "## Table Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c030c89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Migrating Categories → categories...\n",
      "1. Reading from SQL Server...\n",
      "   Read 8 rows\n",
      "\n",
      "2. Preparing data...\n",
      "   Prepared 8 rows\n",
      "\n",
      "3. Processing bulk load...\n",
      "[SUCCESS] → Loaded 8 rows\n",
      "\n",
      "4. Verifying row counts...\n",
      "[SUCCESS] → Verification passed: 8 = 8\n",
      "\n",
      "Categories migration successfully completed.\n",
      "\n",
      "Migrating Suppliers → suppliers...\n",
      "1. Reading from SQL Server...\n",
      "   Read 5,000 rows\n",
      "\n",
      "2. Preparing data...\n",
      "   Prepared 5,000 rows\n",
      "\n",
      "3. Processing bulk load...\n",
      "[SUCCESS] → Loaded 5,000 rows\n",
      "\n",
      "4. Verifying row counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_29484\\861530073.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sql_df = pd.read_sql(extract_query, sql_conn)\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_29484\\861530073.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sql_df = pd.read_sql(extract_query, sql_conn)\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_29484\\861530073.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sql_df = pd.read_sql(extract_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] → Verification passed: 5,000 = 5,000\n",
      "\n",
      "Suppliers migration successfully completed.\n",
      "\n",
      "Migrating Products → products...\n",
      "1. Reading from SQL Server...\n",
      "   Read 150,000 rows\n",
      "\n",
      "2. Preparing data...\n",
      "   Prepared 150,000 rows\n",
      "\n",
      "3. Processing bulk load...\n",
      "[SUCCESS] → Loaded 150,000 rows\n",
      "\n",
      "4. Verifying row counts...\n",
      "[SUCCESS] → Verification passed: 150,000 = 150,000\n",
      "\n",
      "Products migration successfully completed.\n"
     ]
    }
   ],
   "source": [
    "# Exclude Customers table if already migrated\n",
    "remaining_tables = []\n",
    "\n",
    "for t in tables_to_migrate:\n",
    "    if t != \"Customers\":\n",
    "        remaining_tables.append(t)\n",
    "\n",
    "for table in remaining_tables:\n",
    "    pg_table = table.lower()\n",
    "\n",
    "    print(f\"\\nMigrating {table} → {pg_table}...\")\n",
    "\n",
    "    try:\n",
    "        # 1. Read from SQL Server\n",
    "        print(\"1. Reading from SQL Server...\")\n",
    "        extract_query = f\"SELECT * FROM {table}\"\n",
    "        sql_df = pd.read_sql(extract_query, sql_conn)\n",
    "        print(f\"   Read {len(sql_df):,} rows\\n\")\n",
    "\n",
    "        # 2. Prepare data\n",
    "        print(\"2. Preparing data...\")\n",
    "        data_tuples = [tuple(row) for row in sql_df.to_numpy()]\n",
    "\n",
    "        columns = [col.lower() for col in sql_df.columns]\n",
    "        columns_string = \", \".join(columns)\n",
    "\n",
    "        #placeholders = \", \".join([\"%s\"] * len(columns))\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {pg_table} ({columns_string})\n",
    "            VALUES %s\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"   Prepared {len(data_tuples):,} rows\\n\")\n",
    "\n",
    "        # 3. Bulk insert into PostgreSQL\n",
    "        print(\"3. Processing bulk load...\")\n",
    "        execute_values(\n",
    "            pg_cursor,\n",
    "            insert_query,\n",
    "            data_tuples,\n",
    "            page_size=1000\n",
    "        )\n",
    "        pg_conn.commit()\n",
    "\n",
    "        print(f\"[SUCCESS] → Loaded {len(data_tuples):,} rows\\n\")\n",
    "\n",
    "        # 4. Verification\n",
    "        print(\"4. Verifying row counts...\")\n",
    "        pg_cursor.execute(f\"SELECT COUNT(*) FROM {pg_table}\")\n",
    "        pg_count = pg_cursor.fetchone()[0]\n",
    "\n",
    "        sql_count = baseline_counts.get(table)\n",
    "\n",
    "        if pg_count == sql_count:\n",
    "            print(f\"[SUCCESS] → Verification passed: {pg_count:,} = {sql_count:,}\")\n",
    "        else:\n",
    "            print(f\"[FAILED] → Count mismatch: {pg_count:,} ≠ {sql_count:,}\")\n",
    "\n",
    "        print(f\"\\n{table} migration successfully completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        pg_conn.rollback()\n",
    "        print(f\"[ERROR] → Failed migrating {table}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlserver-to-postgres",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
